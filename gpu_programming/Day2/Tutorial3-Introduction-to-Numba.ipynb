{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Python GPU Programming with Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Misc. import\n",
    "from __future__ import print_function\n",
    "from IPython.display import Image \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numba\n",
    "* Opensource BSD license\n",
    "* Basic CUDA GPU JIT compilation\n",
    "* OpenCL support coming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "print(\"numba\", numba.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The CUDA GPU\n",
    "\n",
    "- A massively parallel processor (many cores)\n",
    "    - 100 threads, 1,000 threads, and more\n",
    "- optimized for data throughput\n",
    "    - Simple (shallow) cache hierarchy\n",
    "    - Best with manual caching!\n",
    "    - Cache memory is called shared memory and it is addressable\n",
    "- CPU is latency optimized\n",
    "    - Deep cache hierarchy\n",
    "    - L1, L2, L3 caches\n",
    "- GPU execution model is different\n",
    "- GPU forces you to think and program *in parallel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Get all the imports we need\n",
    "import numba.cuda\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "my_gpu = numba.cuda.get_current_device()\n",
    "print(\"Running on GPU:\", my_gpu.name)\n",
    "cores_per_capability = {\n",
    "    1: 8,\n",
    "    2: 32,\n",
    "    3: 192,\n",
    "    5: 128\n",
    "}\n",
    "cc = my_gpu.compute_capability\n",
    "print(\"Compute capability: \", \"%d.%d\" % cc, \"(Numba requires >= 2.0)\")\n",
    "majorcc = cc[0]\n",
    "print(\"Number of streaming multiprocessor:\", my_gpu.MULTIPROCESSOR_COUNT)\n",
    "cores_per_multiprocessor = cores_per_capability[majorcc]\n",
    "print(\"Number of cores per mutliprocessor:\", cores_per_multiprocessor)\n",
    "total_cores = cores_per_multiprocessor * my_gpu.MULTIPROCESSOR_COUNT\n",
    "print(\"Number of cores on GPU:\", total_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# High-level Array-Oriented Style\n",
    "\n",
    "- Use NumPy array as a unit of computation\n",
    "- Use NumPy universal function (ufunc) as an abstraction of computation and scheduling\n",
    "- ufuncs are elementwise functions\n",
    "- If you use NumPy, you are using ufuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(np.sin, \"is of type\", type(np.sin))\n",
    "print(np.add, \"is of type\", type(np.add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vectorize\n",
    "\n",
    "- generate a ufunc from a python function\n",
    "- converts scalar function to elementwise array function\n",
    "- Numba provides CPU and GPU support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# CPU version\n",
    "@numba.vectorize(['float32(float32, float32)',\n",
    "                  'float64(float64, float64)'], target='cpu')\n",
    "def cpu_sincos(x, y):\n",
    "    return math.sin(x) * math.cos(y)\n",
    "\n",
    "# CUDA version\n",
    "@numba.vectorize(['float32(float32, float32)',\n",
    "                     'float64(float64, float64)'], target='cuda')\n",
    "def gpu_sincos(x, y):\n",
    "    return math.sin(x) * math.cos(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "@numba.vectorize(<list of signatures>, target=<'cpu', 'cuda'>)\n",
    "```\n",
    "\n",
    "- A ufunc can be overloaded to work on multiple type signatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Test it out\n",
    "\n",
    "- 2 input arrays\n",
    "- 1 output array\n",
    "- 1 million doubles (8 MB) per array\n",
    "- Total 24 MB of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "n = 1000000\n",
    "x = np.linspace(0, np.pi, n)\n",
    "y = np.linspace(0, np.pi, n)\n",
    "\n",
    "# Check result\n",
    "np_ans = np.sin(x) * np.cos(y)\n",
    "nb_cpu_ans = cpu_sincos(x, y)\n",
    "nb_gpu_ans = gpu_sincos(x, y)\n",
    "\n",
    "print(\"CPU vectorize correct: \", np.allclose(nb_cpu_ans, np_ans))\n",
    "print(\"GPU vectorize correct: \", np.allclose(nb_gpu_ans, np_ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"NumPy\")\n",
    "%timeit np.sin(x) * np.cos(y)\n",
    "\n",
    "print(\"CPU vectorize\")\n",
    "%timeit cpu_sincos(x, y)\n",
    "\n",
    "print(\"GPU vectorize\")\n",
    "%timeit gpu_sincos(x, y)\n",
    "\n",
    "# Optional cleanup \n",
    "del x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- CPU vectorize time is similar to pure NumPy time because ``sin()`` and ``cos()`` calls dominate the time.\n",
    "- GPU vectorize is a lot faster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Behind the scence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Automatic memory transfer\n",
    "\n",
    "- NumPy arrays are automatically transferred\n",
    "    - CPU -> GPU\n",
    "    - GPU -> CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Automatic work scheduling\n",
    "\n",
    "- The work is distributed the across all threads on the GPU\n",
    "- The GPU hardware handles the scheduling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Automatic GPU memory management\n",
    "\n",
    "- GPU memory is tied to object lifetime\n",
    "- freed automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalized Universal Function (guvectorize)\n",
    "\n",
    "- Vectorize is limited to scalar arguments in the core function\n",
    "- GUVectorize accepts array arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@numba.guvectorize(['void(float32[:,:], float32[:,:], float32[:,:])'],\n",
    "                      '(m, n),(n, p)->(m, p)', target='cuda')\n",
    "def batch_matrix_mult(a, b, c):\n",
    "    for i in range(c.shape[0]):\n",
    "        for j in range(c.shape[1]):\n",
    "            tmp = 0\n",
    "            for n in range(a.shape[1]):\n",
    "                 tmp += a[i, n] * b[n, j]\n",
    "            c[i, j] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "@numba.guvectorize(<list of function signatures>, <a string to desc the shape signature>, target=<'cpu', 'cuda'>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "a = np.arange(1.0, 10.0, dtype=np.float32).reshape(3,3)\n",
    "b = np.arange(1.0, 10.0, dtype=np.float32).reshape(3,3)\n",
    "\n",
    "# Use the builtin matrix_multiply in NumPy for CPU test\n",
    "import numpy.core.umath_tests as ut\n",
    "\n",
    "# Check result\n",
    "print('NumPy result')\n",
    "np_ans = ut.matrix_multiply(a, b)\n",
    "print(np_ans)\n",
    "\n",
    "print('Numba GPU result')\n",
    "gpu_ans = batch_matrix_mult(a, b)\n",
    "print(gpu_ans)\n",
    "\n",
    "assert np.allclose(np_ans, gpu_ans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Test it out\n",
    "\n",
    "- batch multiply two 4 million 2x2 matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 4000000\n",
    "dim = 2\n",
    "a = np.random.random(n * dim * dim).astype(np.float32).reshape(n, dim, dim)\n",
    "b = np.random.random(n * dim * dim).astype(np.float32).reshape(n, dim, dim)\n",
    "\n",
    "print('NumPy time')\n",
    "%timeit ut.matrix_multiply(a, b)\n",
    "\n",
    "print('Numba GPU time')\n",
    "%timeit batch_matrix_mult(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- GPU time seems to be similar to CPU time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Manually Transfer the data to the GPU\n",
    "\n",
    "- This will let us see the actual compute time without the CPU<->GPU transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dc = numba.cuda.device_array_like(a)\n",
    "da = numba.cuda.to_device(a)\n",
    "db = numba.cuda.to_device(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ```numba.cuda.device_array_like``` allocate without initialization with the type and shape of another array.\n",
    "    * similar to ```numpy.empty_like(a)```\n",
    "* ```numba.cuda.to_device``` create a GPU copy of the CPU array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Pure compute time on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def check_pure_compute_time(da, db, dc):\n",
    "    batch_matrix_mult(da, db, out=dc)\n",
    "    numba.cuda.synchronize()   # ensure the call has completed\n",
    "    \n",
    "%timeit check_pure_compute_time(da, db, dc)\n",
    "del da, db, dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Actual compute time is **a lot faster**\n",
    "* PCI-express transfer overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Tips\n",
    "If you have a sequence of ufuncs to apply, pin the data on the GPU by manual transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-----\n",
    "\n",
    "# Low-Level Approach: @numba.cuda.jit\n",
    "\n",
    "- Numba can generate CUDA functions with the `@jit` decorator\n",
    "- Decorated function follows CUDA execution model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CUDA Execution Model\n",
    "\n",
    "- Kernel functions\n",
    "    - visible to the host CPU\n",
    "    - cannot return any value\n",
    "        - use output argument\n",
    "    - associates to a _grid_\n",
    "- Grid\n",
    "    - a group of blocks\n",
    "    - 1D, 2D, 3D\n",
    "- Blocks\n",
    "    - a group of threads\n",
    "    - 1D, 2D, 3D  \n",
    "- Every thread executes the same kernel\n",
    "    - thread can use the grid, block, thread coordinate system to determine its ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Image(url='http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/grid-of-thread-blocks.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Compiling a CUDA Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "@numba.cuda.jit(\"void(float32[:], float32[:], float32[:])\")\n",
    "def vadd(arr_a, arr_b, arr_out):\n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    bw = cuda.blockDim.x    # number of threads per block\n",
    "    i = tx + bx * bw\n",
    "    if i >= arr_out.size:\n",
    "        return\n",
    "    arr_out[i] = arr_a[i] + arr_b[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code Explained\n",
    "\n",
    "#### Define a CUDA kernel with three 1D float32 arrays as args\n",
    "\n",
    "```\n",
    "@numba.cuda.jit(\"void(float32[:], float32[:], float32[:])\")\n",
    "def vadd(arr_a, arr_b, arr_out):\n",
    "```\n",
    "\n",
    "#### Map thread, block coordinate to global position\n",
    "```\n",
    "    tx = cuda.threadIdx.x   # thread label (along x dimension)\n",
    "    bx = cuda.blockIdx.x    # block label (along x dimension)\n",
    "    bw = cuda.blockDim.x    # number of threads in each block (along x dimension)\n",
    "    i = tx + bx * bw        # flattened linear address for each thread\n",
    "```\n",
    "or simplified to:\n",
    "```\n",
    "    i = cuda.grid(1)\n",
    "``` \n",
    "\n",
    "#### Ensure global position is within array size\n",
    "\n",
    "```\n",
    "    if i >= arr_out.size:\n",
    "        return\n",
    "```\n",
    "\n",
    "#### The actual work\n",
    "\n",
    "```\n",
    "    arr_out[i] = arr_a[i] + arr_b[i]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Launch kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "a = np.arange(n, dtype=np.float32)\n",
    "b = np.arange(n, dtype=np.float32)\n",
    "c = np.empty_like(a)                 # Must prepare the output array to hold the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Calculate thread, block count\n",
    "\n",
    "- thread count is set to **warp size** of the GPU\n",
    "    - Warp size is similar to SIMD vector width on the CPU\n",
    "    - **performance tips**: set thread count to multiple of warp size\n",
    "- block count is ceil(n/thread_ct)\n",
    "\n",
    "**Note:**\n",
    "This will launch more threads than there are elements in the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "thread_ct = my_gpu.WARP_SIZE\n",
    "block_ct = int(math.ceil(float(n) / thread_ct))\n",
    "\n",
    "print(\"Threads per block:\", thread_ct)\n",
    "print(\"Block per grid:\", block_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Launch kernel\n",
    "\n",
    "Kernel function object uses the ``__getitem__`` (indexing notation) to configure the grid and block dimensions.\n",
    "\n",
    "```\n",
    "    kernel_function[griddim, blockdim](*args)\n",
    "```\n",
    "\n",
    "- **griddim**\n",
    "    - Number of blocks per grid (grid dimension)\n",
    "    - type: int for 1d or 1,2,3-tuple of ints for 1d, 2d, or 3d respectively\n",
    "\n",
    "- **blockdim**: \n",
    "    - Number of threads per block (blockdim dimension)\n",
    "    - type: int for 1d or 1,2,3-tuple of ints for 1d, 2d, or 3d respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vadd[block_ct, thread_ct](a, b, c)    # Last argument is the output array in this case\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example: Matrix Matrix Multiplication\n",
    "\n",
    "- Show manual caching with shared memory\n",
    "- Not the best matrix matrix multiplication implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Prepare constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from numba import float32\n",
    "\n",
    "bpg = 150\n",
    "tpb = 32\n",
    "n = bpg * tpb\n",
    "shared_mem_size = (tpb, tpb)\n",
    "griddim = bpg, bpg\n",
    "blockdim = tpb, tpb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Naive version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Image(url=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-without-shared-memory.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@numba.cuda.jit(\"void(float32[:,:], float32[:,:], float32[:,:])\")\n",
    "def naive_matrix_mult(A, B, C):\n",
    "    x, y = cuda.grid(2)\n",
    "    if x >= n or y >= n:\n",
    "        return\n",
    "\n",
    "    C[y, x] = 0\n",
    "    for i in range(n):\n",
    "        C[y, x] += A[y, i] * B[i, x]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Optimized version (shared memory + blocking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Image(url=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-with-shared-memory.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@numba.cuda.jit(\"void(float32[:,:], float32[:,:], float32[:,:])\")\n",
    "def optimized_matrix_mult(A, B, C):\n",
    "    # Declare shared memory\n",
    "    sA = cuda.shared.array(shape=shared_mem_size, dtype=float32)\n",
    "    sB = cuda.shared.array(shape=shared_mem_size, dtype=float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    acc = 0\n",
    "    for i in range(bpg):\n",
    "        if x < n and y < n:\n",
    "            # Prefill cache\n",
    "            sA[ty, tx] = A[y, tx + i * tpb]\n",
    "            sB[ty, tx] = B[ty + i * tpb, x]\n",
    "\n",
    "        # Synchronize all threads in the block\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        if x < n and y < n:\n",
    "            # Compute product\n",
    "            for j in range(tpb):\n",
    "                acc += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        # Wait until all threads finish the computation\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    if x < n and y < n:\n",
    "        C[y, x] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data on the CPU\n",
    "A = np.array(np.random.random((n, n)), dtype=np.float32)\n",
    "B = np.array(np.random.random((n, n)), dtype=np.float32)\n",
    "\n",
    "print(\"N = %d x %d\" % (n, n))\n",
    "\n",
    "# Prepare data on the GPU\n",
    "dA = cuda.to_device(A)\n",
    "dB = cuda.to_device(B)\n",
    "dC = cuda.device_array_like(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Time the unoptimized version\n",
    "import time\n",
    "s = time.time()\n",
    "naive_matrix_mult[griddim, blockdim](dA, dB, dC)\n",
    "numba.cuda.synchronize()\n",
    "e = time.time()\n",
    "unopt_ans = dC.copy_to_host()\n",
    "tcuda_unopt = e - s\n",
    "\n",
    "# Time the optimized version\n",
    "s = time.time()\n",
    "optimized_matrix_mult[griddim, blockdim](dA, dB, dC)\n",
    "numba.cuda.synchronize()\n",
    "e = time.time()\n",
    "opt_ans = dC.copy_to_host()\n",
    "tcuda_opt = e - s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(unopt_ans, opt_ans)\n",
    "print(\"Without shared memory:\", \"%.2f\" % tcuda_unopt, \"s\")\n",
    "print(\"With shared memory:\", \"%.2f\" % tcuda_opt, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- Numba\n",
    "    - opensource low-level GPU support\n",
    "    - CUDA kernel ``@numba.cuda.jit``\n",
    "    - vectorize ``@numba.vectorize``\n",
    "    - guvectorize ``@numba.guvectorize``\n",
    "- Accelerate\n",
    "    - CUDA libraries ``accelerate.cuda.*``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
