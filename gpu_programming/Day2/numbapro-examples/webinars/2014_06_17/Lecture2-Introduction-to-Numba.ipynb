{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Python GPU Programming with Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Misc. import\n",
    "from __future__ import print_function\n",
    "from IPython.display import Image \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numba\n",
    "* Opensource BSD license\n",
    "* Basic CUDA GPU JIT compilation\n",
    "* OpenCL support coming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numba 0.33.0\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "print(\"numba\", numba.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The CUDA GPU\n",
    "\n",
    "- A massively parallel processor (many cores)\n",
    "    - 100 threads, 1,000 threads, and more\n",
    "- optimized for data throughput\n",
    "    - Simple (shallow) cache hierarchy\n",
    "    - Best with manual caching!\n",
    "    - Cache memory is called shared memory and it is addressable\n",
    "- CPU is latency optimized\n",
    "    - Deep cache hierarchy\n",
    "    - L1, L2, L3 caches\n",
    "- GPU execution model is different\n",
    "- GPU forces you to think and program *in parallel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU: Tesla K40m\n",
      "Compute capability:  3.5 (Numba requires >= 2.0)\n",
      "Number of streaming multiprocessor: 15\n",
      "Number of cores per mutliprocessor: 192\n",
      "Number of cores on GPU: 2880\n"
     ]
    }
   ],
   "source": [
    "# Get all the imports we need\n",
    "import numba.cuda\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "my_gpu = numba.cuda.get_current_device()\n",
    "print(\"Running on GPU:\", my_gpu.name)\n",
    "cores_per_capability = {\n",
    "    1: 8,\n",
    "    2: 32,\n",
    "    3: 192,\n",
    "    5: 128\n",
    "}\n",
    "cc = my_gpu.compute_capability\n",
    "print(\"Compute capability: \", \"%d.%d\" % cc, \"(Numba requires >= 2.0)\")\n",
    "majorcc = cc[0]\n",
    "print(\"Number of streaming multiprocessor:\", my_gpu.MULTIPROCESSOR_COUNT)\n",
    "cores_per_multiprocessor = cores_per_capability[majorcc]\n",
    "print(\"Number of cores per mutliprocessor:\", cores_per_multiprocessor)\n",
    "total_cores = cores_per_multiprocessor * my_gpu.MULTIPROCESSOR_COUNT\n",
    "print(\"Number of cores on GPU:\", total_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# High-level Array-Oriented Style\n",
    "\n",
    "- Use NumPy array as a unit of computation\n",
    "- Use NumPy universal function (ufunc) as an abstraction of computation and scheduling\n",
    "- ufuncs are elementwise functions\n",
    "- If you use NumPy, you are using ufuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ufunc 'sin'> is of type <type 'numpy.ufunc'>\n",
      "<ufunc 'add'> is of type <type 'numpy.ufunc'>\n"
     ]
    }
   ],
   "source": [
    "print(np.sin, \"is of type\", type(np.sin))\n",
    "print(np.add, \"is of type\", type(np.add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vectorize\n",
    "\n",
    "- generate a ufunc from a python function\n",
    "- converts scalar function to elementwise array function\n",
    "- Numba provides CPU and GPU support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# CPU version\n",
    "@numba.vectorize(['float32(float32, float32)',\n",
    "                  'float64(float64, float64)'], target='cpu')\n",
    "def cpu_sincos(x, y):\n",
    "    return math.sin(x) * math.cos(y)\n",
    "\n",
    "# CUDA version\n",
    "@numba.vectorize(['float32(float32, float32)',\n",
    "                     'float64(float64, float64)'], target='cuda')\n",
    "def gpu_sincos(x, y):\n",
    "    return math.sin(x) * math.cos(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "@numba.vectorize(<list of signatures>, target=<'cpu', 'gpu'>)\n",
    "```\n",
    "\n",
    "- A ufunc can be overloaded to work on multiple type signatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Test it out\n",
    "\n",
    "- 2 input arrays\n",
    "- 1 output array\n",
    "- 1 million doubles (8 MB) per array\n",
    "- Total 24 MB of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU vectorize correct:  True\n",
      "GPU vectorize correct:  True\n"
     ]
    }
   ],
   "source": [
    "# Generate data\n",
    "n = 1000000\n",
    "x = np.linspace(0, np.pi, n)\n",
    "y = np.linspace(0, np.pi, n)\n",
    "\n",
    "# Check result\n",
    "np_ans = np.sin(x) * np.cos(y)\n",
    "nb_cpu_ans = cpu_sincos(x, y)\n",
    "nb_gpu_ans = gpu_sincos(x, y)\n",
    "\n",
    "print(\"CPU vectorize correct: \", np.allclose(nb_cpu_ans, np_ans))\n",
    "print(\"GPU vectorize correct: \", np.allclose(nb_gpu_ans, np_ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy\n",
      "10 loops, best of 3: 72.3 ms per loop\n",
      "CPU vectorize\n",
      "10 loops, best of 3: 63.7 ms per loop\n",
      "GPU vectorize\n",
      "100 loops, best of 3: 17.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy\")\n",
    "%timeit np.sin(x) * np.cos(y)\n",
    "\n",
    "print(\"CPU vectorize\")\n",
    "%timeit cpu_sincos(x, y)\n",
    "\n",
    "print(\"GPU vectorize\")\n",
    "%timeit gpu_sincos(x, y)\n",
    "\n",
    "# Optional cleanup \n",
    "del x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- CPU vectorize time is similar to pure NumPy time because ``sin()`` and ``cos()`` calls dominate the time.\n",
    "- GPU vectorize is a lot faster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Behind the scence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Automatic memory transfer\n",
    "\n",
    "- NumPy arrays are automatically transferred\n",
    "    - CPU -> GPU\n",
    "    - GPU -> CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Automatic work scheduling\n",
    "\n",
    "- The work is distributed the across all threads on the GPU\n",
    "- The GPU hardware handles the scheduling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Automatic GPU memory management\n",
    "\n",
    "- GPU memory is tied to object lifetime\n",
    "- freed automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalized Universal Function (guvectorize)\n",
    "\n",
    "- Vectorize is limited to scalar arguments in the core function\n",
    "- GUVectorize accepts array arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@numba.guvectorize(['void(float32[:,:], float32[:,:], float32[:,:])'],\n",
    "                      '(m, n),(n, p)->(m, p)', target='cuda')\n",
    "def batch_matrix_mult(a, b, c):\n",
    "    for i in range(c.shape[0]):\n",
    "        for j in range(c.shape[1]):\n",
    "            tmp = 0\n",
    "            for n in range(a.shape[1]):\n",
    "                 tmp += a[i, n] * b[n, j]\n",
    "            c[i, j] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "@numba.guvectorize(<list of function signatures>, <a string to desc the shape signature>, target=<'cpu', 'cuda'>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy result\n",
      "[[  30.   36.   42.]\n",
      " [  66.   81.   96.]\n",
      " [ 102.  126.  150.]]\n",
      "Numba GPU result\n",
      "[[  30.   36.   42.]\n",
      " [  66.   81.   96.]\n",
      " [ 102.  126.  150.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(1.0, 10.0, dtype=np.float32).reshape(3,3)\n",
    "b = np.arange(1.0, 10.0, dtype=np.float32).reshape(3,3)\n",
    "\n",
    "# Use the builtin matrix_multiply in NumPy for CPU test\n",
    "import numpy.core.umath_tests as ut\n",
    "\n",
    "# Check result\n",
    "print('NumPy result')\n",
    "np_ans = ut.matrix_multiply(a, b)\n",
    "print(np_ans)\n",
    "\n",
    "print('Numba GPU result')\n",
    "gpu_ans = batch_matrix_mult(a, b)\n",
    "print(gpu_ans)\n",
    "\n",
    "assert np.allclose(np_ans, gpu_ans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Test it out\n",
    "\n",
    "- batch multiply two 4 million 2x2 matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy time\n",
      "10 loops, best of 3: 139 ms per loop\n",
      "Numba GPU time\n",
      "1 loop, best of 3: 220 ms per loop\n"
     ]
    }
   ],
   "source": [
    "n = 4000000\n",
    "dim = 2\n",
    "a = np.random.random(n * dim * dim).astype(np.float32).reshape(n, dim, dim)\n",
    "b = np.random.random(n * dim * dim).astype(np.float32).reshape(n, dim, dim)\n",
    "\n",
    "print('NumPy time')\n",
    "%timeit ut.matrix_multiply(a, b)\n",
    "\n",
    "print('Numba GPU time')\n",
    "%timeit batch_matrix_mult(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- GPU time seems to be similar to CPU time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Manually Transfer the data to the GPU\n",
    "\n",
    "- This will let us see the actual compute time without the CPU<->GPU transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dc = numba.cuda.device_array_like(a)\n",
    "da = numba.cuda.to_device(a)\n",
    "db = numba.cuda.to_device(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ```numba.cuda.device_array_like``` allocate without initialization with the type and shape of another array.\n",
    "    * similar to ```numpy.empty_like(a)```\n",
    "* ```numba.cuda.to_device``` create a GPU copy of the CPU array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Pure compute time on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 3.27 ms per loop\n"
     ]
    }
   ],
   "source": [
    "def check_pure_compute_time(da, db, dc):\n",
    "    batch_matrix_mult(da, db, out=dc)\n",
    "    numba.cuda.synchronize()   # ensure the call has completed\n",
    "    \n",
    "%timeit check_pure_compute_time(da, db, dc)\n",
    "del da, db, dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Actual compute time is **a lot faster**\n",
    "* PCI-express transfer overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Tips\n",
    "If you have a sequence of ufuncs to apply, pin the data on the GPU by manual transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Accelerate CUDA Libraries Bindings\n",
    "\n",
    "- Access to CUDA libraries \n",
    "- Work seamless with NumPy\n",
    "    - auto memory transfer\n",
    "    - managed memory\n",
    "\n",
    "- cuBLAS: CUDA version of BLAS\n",
    "- cuSparse: CUDA sparse matrix support\n",
    "- cuFFT: FFT on CUDA\n",
    "- cuRAND: random number generation on CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-----\n",
    "\n",
    "# Low-Level Approach: @numba.cuda.jit\n",
    "\n",
    "- Numba can generate CUDA functions with the `@jit` decorator\n",
    "- Decorated function follows CUDA execution model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CUDA Execution Model\n",
    "\n",
    "- Kernel functions\n",
    "    - visible to the host CPU\n",
    "    - cannot return any value\n",
    "        - use output argument\n",
    "    - associates to a _grid_\n",
    "- Grid\n",
    "    - a group of blocks\n",
    "    - 1D, 2D, 3D\n",
    "- Blocks\n",
    "    - a group of threads\n",
    "    - 1D, 2D, 3D  \n",
    "- Every thread executes the same kernel\n",
    "    - thread can use the grid, block, thread coordinate system to determine its ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/grid-of-thread-blocks.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/grid-of-thread-blocks.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Compiling a CUDA Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "@numba.cuda.jit(\"void(float32[:], float32[:], float32[:])\")\n",
    "def vadd(arr_a, arr_b, arr_out):\n",
    "    tx = cuda.threadIdx.x\n",
    "    bx = cuda.blockIdx.x\n",
    "    bw = cuda.blockDim.x    # number of threads per block\n",
    "    i = tx + bx * bw\n",
    "    if i >= arr_out.size:\n",
    "        return\n",
    "    arr_out[i] = arr_a[i] + arr_b[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code Explained\n",
    "\n",
    "#### Define a CUDA kernel with three 1D float32 arrays as args\n",
    "\n",
    "```\n",
    "@numba.cuda.jit(\"void(float32[:], float32[:], float32[:])\")\n",
    "def vadd(arr_a, arr_b, arr_out):\n",
    "```\n",
    "\n",
    "#### Map thread, block coordinate to global position\n",
    "```\n",
    "    tx = cuda.threadIdx.x   # thread label (along x dimension)\n",
    "    bx = cuda.blockIdx.x    # block label (along x dimension)\n",
    "    bw = cuda.blockDim.x    # number of threads in each block (along x dimension)\n",
    "    i = tx + bx * bw        # flattened linear address for each thread\n",
    "```\n",
    "or simplified to:\n",
    "```\n",
    "    i = cuda.grid(1)\n",
    "``` \n",
    "\n",
    "#### Ensure global position is within array size\n",
    "\n",
    "```\n",
    "    if i >= arr_out.size:\n",
    "        return\n",
    "```\n",
    "\n",
    "#### The actual work\n",
    "\n",
    "```\n",
    "    arr_out[i] = arr_a[i] + arr_b[i]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Launch kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "a = np.arange(n, dtype=np.float32)\n",
    "b = np.arange(n, dtype=np.float32)\n",
    "c = np.empty_like(a)                 # Must prepare the output array to hold the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Calculate thread, block count\n",
    "\n",
    "- thread count is set to **warp size** of the GPU\n",
    "    - Warp size is similar to SIMD vector width on the CPU\n",
    "    - **performance tips**: set thread count to multiple of warp size\n",
    "- block count is ceil(n/thread_ct)\n",
    "\n",
    "**Note:**\n",
    "This will launch more threads than there are elements in the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threads per block: 32\n",
      "Block per grid: 4\n"
     ]
    }
   ],
   "source": [
    "thread_ct = my_gpu.WARP_SIZE\n",
    "block_ct = int(math.ceil(float(n) / thread_ct))\n",
    "\n",
    "print(\"Threads per block:\", thread_ct)\n",
    "print(\"Block per grid:\", block_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Launch kernel\n",
    "\n",
    "Kernel function object uses the ``__getitem__`` (indexing notation) to configure the grid and block dimensions.\n",
    "\n",
    "```\n",
    "    kernel_function[griddim, blockdim](*args)\n",
    "```\n",
    "\n",
    "- **griddim**\n",
    "    - Number of blocks per grid (grid dimension)\n",
    "    - type: int for 1d or 1,2,3-tuple of ints for 1d, 2d, or 3d respectively\n",
    "\n",
    "- **blockdim**: \n",
    "    - Number of threads per block (blockdim dimension)\n",
    "    - type: int for 1d or 1,2,3-tuple of ints for 1d, 2d, or 3d respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.    2.    4.    6.    8.   10.   12.   14.   16.   18.   20.   22.\n",
      "   24.   26.   28.   30.   32.   34.   36.   38.   40.   42.   44.   46.\n",
      "   48.   50.   52.   54.   56.   58.   60.   62.   64.   66.   68.   70.\n",
      "   72.   74.   76.   78.   80.   82.   84.   86.   88.   90.   92.   94.\n",
      "   96.   98.  100.  102.  104.  106.  108.  110.  112.  114.  116.  118.\n",
      "  120.  122.  124.  126.  128.  130.  132.  134.  136.  138.  140.  142.\n",
      "  144.  146.  148.  150.  152.  154.  156.  158.  160.  162.  164.  166.\n",
      "  168.  170.  172.  174.  176.  178.  180.  182.  184.  186.  188.  190.\n",
      "  192.  194.  196.  198.]\n"
     ]
    }
   ],
   "source": [
    "vadd[block_ct, thread_ct](a, b, c)    # Last argument is the output array in this case\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example: Matrix Matrix Multiplication\n",
    "\n",
    "- Show manual caching with shared memory\n",
    "- Not the best matrix matrix multiplication implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Prepare constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from numba import float32\n",
    "\n",
    "bpg = 150\n",
    "tpb = 32\n",
    "n = bpg * tpb\n",
    "shared_mem_size = (tpb, tpb)\n",
    "griddim = bpg, bpg\n",
    "blockdim = tpb, tpb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Naive version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-without-shared-memory.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-without-shared-memory.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@numba.cuda.jit(\"void(float32[:,:], float32[:,:], float32[:,:])\")\n",
    "def naive_matrix_mult(A, B, C):\n",
    "    x, y = cuda.grid(2)\n",
    "    if x >= n or y >= n:\n",
    "        return\n",
    "\n",
    "    C[y, x] = 0\n",
    "    for i in range(n):\n",
    "        C[y, x] += A[y, i] * B[i, x]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Optimized version (shared memory + blocking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-with-shared-memory.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/matrix-multiplication-with-shared-memory.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "@numba.cuda.jit(\"void(float32[:,:], float32[:,:], float32[:,:])\")\n",
    "def optimized_matrix_mult(A, B, C):\n",
    "    # Declare shared memory\n",
    "    sA = cuda.shared.array(shape=shared_mem_size, dtype=float32)\n",
    "    sB = cuda.shared.array(shape=shared_mem_size, dtype=float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    acc = 0\n",
    "    for i in range(bpg):\n",
    "        if x < n and y < n:\n",
    "            # Prefill cache\n",
    "            sA[ty, tx] = A[y, tx + i * tpb]\n",
    "            sB[ty, tx] = B[ty + i * tpb, x]\n",
    "\n",
    "        # Synchronize all threads in the block\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        if x < n and y < n:\n",
    "            # Compute product\n",
    "            for j in range(tpb):\n",
    "                acc += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        # Wait until all threads finish the computation\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    if x < n and y < n:\n",
    "        C[y, x] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 4800 x 4800\n"
     ]
    }
   ],
   "source": [
    "# Prepare data on the CPU\n",
    "A = np.array(np.random.random((n, n)), dtype=np.float32)\n",
    "B = np.array(np.random.random((n, n)), dtype=np.float32)\n",
    "\n",
    "print(\"N = %d x %d\" % (n, n))\n",
    "\n",
    "# Prepare data on the GPU\n",
    "dA = cuda.to_device(A)\n",
    "dB = cuda.to_device(B)\n",
    "dC = cuda.device_array_like(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Time the unoptimized version\n",
    "import time\n",
    "s = time.time()\n",
    "naive_matrix_mult[griddim, blockdim](dA, dB, dC)\n",
    "numba.cuda.synchronize()\n",
    "e = time.time()\n",
    "unopt_ans = dC.copy_to_host()\n",
    "tcuda_unopt = e - s\n",
    "\n",
    "# Time the optimized version\n",
    "s = time.time()\n",
    "optimized_matrix_mult[griddim, blockdim](dA, dB, dC)\n",
    "numba.cuda.synchronize()\n",
    "e = time.time()\n",
    "opt_ans = dC.copy_to_host()\n",
    "tcuda_opt = e - s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without shared memory: 3.86 s\n",
      "With shared memory: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "assert np.allclose(unopt_ans, opt_ans)\n",
    "print(\"Without shared memory:\", \"%.2f\" % tcuda_unopt, \"s\")\n",
    "print(\"With shared memory:\", \"%.2f\" % tcuda_opt, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- Numba\n",
    "    - opensource low-level GPU support\n",
    "    - CUDA kernel ``@numba.cuda.jit``\n",
    "    - vectorize ``@numba.vectorize``\n",
    "    - guvectorize ``@numba.guvectorize``\n",
    "- Accelerate\n",
    "    - CUDA libraries ``accelerate.cuda.*``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit, cuda, autojit, float32, float64, f4, f8\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "dtype = np.float64\n",
    "n = 16384\n",
    "blksize = 128\n",
    "eps = 1.e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def induced_velocity3(x, xvort, gam, vel):\n",
    "    nx = x.shape[0]\n",
    "    nvort = xvort.shape[0]\n",
    "    for i in range(nx):\n",
    "        vel[i,0] = 0.\n",
    "        vel[i,1] = 0.\n",
    "        for j in range(nvort):\n",
    "            rsq = (x[i,0]-xvort[j,0])**2 + (x[i,1]-xvort[j,1])**2 + eps**2\n",
    "            vel[i,0] += gam[j] * (x[i,1]-xvort[j,1]) / rsq\n",
    "            vel[i,1] += -gam[j] * (x[i,0]-xvort[j,0]) / rs\n",
    "            \n",
    "def induced_velocity_single(x, xvort, gam):\n",
    "    r = x - xvort\n",
    "    rsq = np.sum(r * r, 1) + eps**2\n",
    "    vel = np.transpose(np.array([r[:,1], -r[:,0]]))\n",
    "    vel = gam * vel / rsq[:,np.newaxis]\n",
    "    return vel\n",
    "\n",
    "def induced_velocity(x, xvort, gam, vel):\n",
    "    vel[:] = 0.\n",
    "    for xv, g in zip(xvort, gam):\n",
    "        r = x - xv\n",
    "        rsq = np.sum(r * r, 1) + eps**2\n",
    "        vel += g * np.transpose(np.array([r[:,1], -r[:,0]])) / rsq[:, np.newaxis]\n",
    "\n",
    "\n",
    "@jit('void(f8[:,:], f8[:,:], f8[:], f8[:,:])', target='cpu')\n",
    "def r_squared(x1, x2, xv1, xv2):\n",
    "    return (x1-xv1)**2 + (x2-xv2)**2 + eps**2\n",
    "\n",
    "@autojit(target='cpu')\n",
    "def induced_velocity2(x, xvort, gam, vel):\n",
    "    nx = x.shape[0]\n",
    "    nvort = xvort.shape[0]\n",
    "    for i in range(nx):\n",
    "        vel[i,0] = 0.\n",
    "        vel[i,1] = 0.\n",
    "        for j in range(nvort):\n",
    "            rsq = (x[i,0]-xvort[j,0])**2 + (x[i,1]-xvort[j,1])**2 + eps**2\n",
    "            vel[i,0] += gam[j] * (x[i,1]-xvort[j,1]) / rsq\n",
    "            vel[i,1] += -gam[j] * (x[i,0]-xvort[j,0]) / rsq\n",
    "\n",
    "\n",
    "@jit('void(f8[:,:], f8[:,:], f8[:], f8[:,:])', target='cpu')\n",
    "def induced_velocity2_test(x, xvort, gam, vel):\n",
    "    # eps = 1.e-2\n",
    "    nx = x.shape[0]\n",
    "    nvort = xvort.shape[0]\n",
    "    for i in range(nx):\n",
    "        vel[i,0] = 0.\n",
    "        vel[i,1] = 0.\n",
    "        for j in range(nvort):\n",
    "            rsq = r_squared(x[i,0], x[i,1], xvort[j,0], xvort[j,1])\n",
    "            vel[i,0] += gam[j] * (x[i,1]-xvort[j,1]) / rsq\n",
    "            vel[i,1] += -gam[j] * (x[i,0]-xvort[j,0]) / rsq\n",
    "\n",
    "\n",
    "@cuda.jit('f8[:,:], f8[:,:], f8[:], f8[:,:]')\n",
    "def induced_velocity3(x, xvort, gam, vel):\n",
    "    # eps = float32(1.e-2)\n",
    "    # i, j = cuda.grid(2)\n",
    "    i = cuda.grid(1)\n",
    "    if i < x.shape[0]:\n",
    "        vel[i,0] = float32(0.)\n",
    "        vel[i,1] = float32(0.)\n",
    "        nvort = xvort.shape[0]\n",
    "        for j in range(nvort):\n",
    "            rsq = (x[i,0]-xvort[j,0])**2 + (x[i,1]-xvort[j,1])**2 + eps**2\n",
    "            vel[i,0] += gam[j] * (x[i,1]-xvort[j,1]) / rsq\n",
    "            vel[i,1] += -gam[j] * (x[i,0]-xvort[j,0]) / rsq\n",
    "\n",
    "@cuda.jit('f8[:,:], f8[:,:], f8[:], f8[:,:]')\n",
    "def induced_velocity4(x, xvort, gam, vel):\n",
    "    smem = cuda.shared.array((blksize, 3), dtype=f8)\n",
    "    t = cuda.threadIdx.x\n",
    "    i = cuda.grid(1)\n",
    "    # eps = 1.e-2\n",
    "    nvort = xvort.shape[0]\n",
    "    nx = x.shape[0]\n",
    "    if i < nx:\n",
    "        x0 = x[i,0]\n",
    "        x1 = x[i,1]\n",
    "    xvel = 0\n",
    "    yvel = 0\n",
    "    nvort = xvort.shape[0]\n",
    "    for blk in range((nvort - 1) // blksize + 1):\n",
    "        # load vortex positions and strengths into shared memory\n",
    "        j = blk * blksize + t\n",
    "        if j < nvort:\n",
    "            smem[t,0] = xvort[j,0]\n",
    "            smem[t,1] = xvort[j,1]\n",
    "            smem[t,2] = gam[j]\n",
    "        else:\n",
    "            smem[t,0] = 0\n",
    "            smem[t,1] = 0\n",
    "            smem[t,2] = 0\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # compute the contributions to the velocity\n",
    "        for k in range(blksize):\n",
    "            rsq = (x0-smem[k,0])**2 + (x1-smem[k,1])**2 + eps**2\n",
    "            xvel +=  smem[k,2] * (x1-smem[k,1]) / rsq\n",
    "            yvel += -smem[k,2] * (x0-smem[k,0]) / rsq\n",
    "        cuda.syncthreads()\n",
    "    if i < nx:\n",
    "        vel[i,0] = xvel\n",
    "        vel[i,1] = yvel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 16384\n",
      "=================Numpy==================\n",
      "Time: 15.638716 seconds\n",
      "=================Numba==================\n",
      "Time: 3.346818 seconds\n",
      "Difference: 0.000000\n",
      "Speedup: 4.672712\n",
      "==================GPU===================\n",
      "Time: 0.069850 seconds\n",
      "Difference: 0.000000\n",
      "Speedup: 223.890095\n",
      "================GPU smem================\n",
      "Time: 0.018489 seconds\n",
      "Difference: 0.000000\n",
      "Speedup: 845.844238\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    vort = np.array(np.random.rand(2*n), dtype=dtype).reshape((n,2))\n",
    "    gamma = np.array(np.random.rand(n), dtype=dtype)\n",
    "    vel = np.zeros_like(vort)\n",
    "    start = timer()\n",
    "    induced_velocity(vort, vort, gamma, vel)\n",
    "    numpy_time = timer() - start\n",
    "    print(\"n = %d\" % n)\n",
    "    print(\"Numpy\".center(40, \"=\"))\n",
    "    print(\"Time: %f seconds\" % numpy_time)\n",
    "\n",
    "    vel2 = np.zeros_like(vort)\n",
    "    start = timer()\n",
    "    induced_velocity2(vort, vort, gamma, vel2)\n",
    "    numba_time = timer() - start\n",
    "    print(\"Numba\".center(40, \"=\"))\n",
    "    print(\"Time: %f seconds\" % numba_time)\n",
    "    error = np.max(np.max(np.abs(vel2 - vel)))\n",
    "    print(\"Difference: %f\" % error)\n",
    "    print(\"Speedup: %f\" % (numpy_time / numba_time))\n",
    "\n",
    "    stream = cuda.stream()\n",
    "    d_vort = cuda.to_device(vort, stream)\n",
    "    d_gamma = cuda.to_device(gamma, stream)\n",
    "    vel3 = np.zeros_like(vort)\n",
    "    d_vel = cuda.to_device(vel3, stream)\n",
    "    # blockdim = (32,32)\n",
    "    # griddim = (n // blockdim[0], n // blockdim[1])\n",
    "    griddim = (n - 1) // blksize + 1\n",
    "    start = timer()\n",
    "    induced_velocity3[griddim, blksize, stream](d_vort, d_vort, d_gamma, d_vel)\n",
    "    d_vel.to_host(stream)\n",
    "    gpu_time = timer() - start\n",
    "    error = np.max(np.max(np.abs(vel3 - vel)))\n",
    "    print(\"GPU\".center(40, \"=\"))\n",
    "    print(\"Time: %f seconds\" % gpu_time)\n",
    "    print(\"Difference: %f\" % error)\n",
    "    print(\"Speedup: %f\" % (numpy_time / gpu_time))\n",
    "    # print(vel3)\n",
    "\n",
    "    vel4 = np.zeros_like(vort)\n",
    "    d_vel2 = cuda.to_device(vel4, stream)\n",
    "    start = timer()\n",
    "    induced_velocity4[griddim, blksize, stream](d_vort, d_vort, d_gamma, d_vel2)\n",
    "    d_vel2.to_host(stream)\n",
    "    gpu2_time = timer() - start\n",
    "    error = np.max(np.max(np.abs(vel4 - vel)))\n",
    "    print(\"GPU smem\".center(40, \"=\"))\n",
    "    print(\"Time: %f seconds\" % gpu2_time)\n",
    "    print(\"Difference: %f\" % error)\n",
    "    print(\"Speedup: %f\" % (numpy_time / gpu2_time))\n",
    "    # print(\"Expected\".center(40,'-'))\n",
    "    # print(vel)\n",
    "    # print(\"GPU\".center(40,'-'))\n",
    "    # print(vel4)\n",
    "    # print(\"Difference\".center(40,'-'))\n",
    "    # print(vel4 - vel)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
