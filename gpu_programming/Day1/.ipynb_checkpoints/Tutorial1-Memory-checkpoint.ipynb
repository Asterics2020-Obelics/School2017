{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CUDA and the GPU memory hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> Up to this point we have only seen and used the GPU global memory, which is the slowest memory from the GPU side. As we will see below there are several layers of memory that can be used during the execution of our kernels.\n",
    "\n",
    ">![Alt text](images/memLayers.png)\n",
    ">![Alt text](images/memCUDA2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> Some specifications of these memories during the execution of the **kernel** are:\n",
    "\n",
    "\n",
    "![Alt text](images/cudaMem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It is important to note that each of these memories has different characteristics such as size, bandidth, latency, etc. The way to declare these memories within our kernel is the following:\n",
    "![Alt text](images/defMem.png)\n",
    "The access speeds are estimated below:\n",
    "![Alt text](images/costMem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> As you may have noticed, Texture memory has not been mentioned. This type of memory is linked to another CUDA structure called CUDA Arrays. These CUDA objects offer a structured way of generating arrays in memory (1D, 2D, 3D) , The disadvantage is that they are local memory spaces that we can not access directly inside our kernels. The reading of the information contained in CUDA Arrays is given only through the references known as Textures. CUDA Arrays can only be written by two methods, Surface Textures or copy between memory banks (Host or Device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will now see how to use the various memories, although the choice between which to use will depend exclusively on the problem to be solved. Let us simply compare each of the memories against the Global Memory by measuring the kernel's execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pycuda import gpuarray\n",
    "from pycuda.compiler import SourceModule\n",
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.tools import DeviceData\n",
    "from pycuda.tools import OccupancyRecord as occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Part of what we have not seen, is to choose the GPU with which to work and generate a channel of communication (or context), we use the following function for that effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from CUDATools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "ctx,device = setDevice()\n",
    "devdata = DeviceData(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "def getKernelInfo(kernel,nthreads, rt=True):\n",
    "    ''' This function returns info about kernels theoretical performance, but warning is not trivial to optimize! '''\n",
    "    shared=kernel.shared_size_bytes\n",
    "    regs=kernel.num_regs\n",
    "    local=kernel.local_size_bytes\n",
    "    const=kernel.const_size_bytes\n",
    "    mbpt=kernel.max_threads_per_block\n",
    "    #threads =  #self.block_size_x* self.block_size_y* self.block_size_z\n",
    "    occupy = occupancy(devdata, nthreads, shared_mem=shared, registers=regs)\n",
    "    print \"==Kernel Memory==\"\n",
    "    print(\"\"\"Local:        {0}\n",
    "Shared:       {1}\n",
    "Registers:    {2}\n",
    "Const:        {3}\n",
    "Max Threads/B:{4}\"\"\".format(local,shared,regs,const,mbpt))\n",
    "    print \"==Occupancy==\"\n",
    "    print(\"\"\"Blocks executed by MP: {0}\n",
    "Limited by:            {1}\n",
    "Warps executed by MP:  {2}\n",
    "Occupancy:             {3}\"\"\".format(occupy.tb_per_mp,occupy.limited_by,occupy.warps_per_mp,occupy.occupancy))\n",
    "    if rt:\n",
    "        return occupy.occupancy\n",
    "    \n",
    "def gpuMesureTime(myKernel, ntimes=1000):\n",
    "    start = cuda.Event()\n",
    "    end = cuda.Event()\n",
    "    start.record()\n",
    "    for i in range(ntimes):\n",
    "      myKernel()\n",
    "    end.record()\n",
    "    end.synchronize()\n",
    "    timeGPU = start.time_till(end)*1e-3\n",
    "    print \"Call the function {0} times takes in GPU {1} seconds.\\n\".format(ntimes,timeGPU)\n",
    "    print \"{0} seconds per call\".format(timeGPU/ntimes)\n",
    "    return timeGPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If there is more than one GPU in the system (it is the case on Cartesius), the function is designed to ask which GPU to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Global memory vs. Registers vs. Shared memory vs. Constant memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> Below we outline a simple example of how to declare and use different types of memory within PyCUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Definimos los kernels \n",
    "Reg_Glob_RW = '''\n",
    "__device__ __constant__ cuPres cMem=3.15149; // You can directly define constant memory \n",
    "                                                        \n",
    "__global__ void rwRegisters(cuPres *A){\n",
    "\n",
    "int tid_x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "int tid_y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "int tid   = gridDim.x * blockDim.x * tid_y + tid_x;\n",
    "\n",
    "\n",
    "cuPres pi = 3.141589; // Register memory write\n",
    "A[tid] = pi; // Register lecture and Global Memory write\n",
    "\n",
    "}\n",
    "\n",
    "__global__ void rwGlobal(cuPres *A, cuPres *B){\n",
    "\n",
    "int tid_x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "int tid_y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "int tid   = gridDim.x * blockDim.x * tid_y + tid_x;\n",
    "\n",
    "B[tid] = 3.141589cuStr; // Global memory write\n",
    "A[tid] = B[tid]; // Global memory read and Global Memory write\n",
    "\n",
    "}\n",
    "\n",
    "__global__ void rwShared(cuPres *A){\n",
    "\n",
    "int tid_x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "int tid_y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "int tid   = gridDim.x * blockDim.x * tid_y + tid_x;\n",
    "__shared__ cuPres pi; \n",
    "pi = 3.141589cuStr; // Shared memory write\n",
    "//__syncthreads();\n",
    "A[tid] = pi; // Shared memory read and Global Memory write\n",
    "}\n",
    "\n",
    "__global__ void rwSharedSync(cuPres *A){\n",
    "\n",
    "int tid_x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "int tid_y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "int tid   = gridDim.x * blockDim.x * tid_y + tid_x;\n",
    "__shared__ cuPres pi; \n",
    "pi = 3.141589cuStr; // Shared memory write\n",
    "__syncthreads();\n",
    "A[tid] = pi; // Shared memory read and Global Memory write\n",
    "}\n",
    "\n",
    "__global__ void rwConstant(cuPres *A){\n",
    "\n",
    "int tid_x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "int tid_y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "int tid   = gridDim.x * blockDim.x * tid_y + tid_x;\n",
    "\n",
    "A[tid] = cMem; // Constant memory read and Global Memory write\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## PRECISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#presCPU, presGPU = np.float32, 'float'\n",
    "presCPU, presGPU = np.float64, 'double'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def optKernels(kFile,pres='float',subBlGr = False, cuB=(1,1,1), cuG=(1,1,1)):\n",
    "    kFile = kFile.replace('cuPres', pres)\n",
    "    cString = 'f'\n",
    "    if pres == 'double': cString = ''\n",
    "    kFile = kFile.replace('cuStr', cString)\n",
    "    if subBlGr: \n",
    "        downVar = ['blockDim.x','blockDim.y','blockDim.z','gridDim.x','gridDim.y','gridDim.z']\n",
    "        upVar      = [str(cuB[0]),str(cuB[1]),str(cuB[2]),\n",
    "                      str(cuG[0]),str(cuG[1]),str(cuG[2])]\n",
    "        dicVarOptim = dict(zip(downVar,upVar))\n",
    "        for i in downVar:\n",
    "            kFile = kFile.replace(i,dicVarOptim[i])\n",
    "    return kFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "myKern = optKernels(Reg_Glob_RW,pres=presGPU,subBlGr=True)\n",
    "print myKern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cuCodeRG = SourceModule(myKern)\n",
    "regRW = cuCodeRG.get_function(\"rwRegisters\") \n",
    "gloRW = cuCodeRG.get_function(\"rwGlobal\")\n",
    "shaRW = cuCodeRG.get_function(\"rwShared\")\n",
    "shaSyRW = cuCodeRG.get_function(\"rwSharedSync\")\n",
    "conRW = cuCodeRG.get_function('rwConstant')\n",
    "#conMemRW = cuCodeRG.get_global('cMem')[0] #We get a pointer to the constant memory address declared in the kernel\n",
    "regRW.prepare('P')\n",
    "gloRW.prepare('PP')\n",
    "shaRW.prepare('P')\n",
    "shaSyRW.prepare('P')\n",
    "conRW.prepare('P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    ">Note some differences between the use of each of the different memories, **Registers** do not present any difference to what we have been using so far, **Global** is not new, except that it is necessary to use two arrays in different global memory spaces. The **Shared** memory however requires an extra ingredient. As you will notice, there are two versions; The first one is asynchronous, while the second performs synchronization within the block. The line that changes between the two versions is *__syncthread()*. This command forces the kernel execution to wait for all threads within the block to have finished the tasks up to this point. The *advantage* of this function is that it allows to coordinate the execution of kernel tasks, while the *disadvantage* being a somewhat slower execution speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now evaluate the performance of the various implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nLoop = 1000\n",
    "timeReg = []\n",
    "timeGlo = []\n",
    "timeSha = []\n",
    "timeShaSy = []\n",
    "timeCons = []\n",
    "timeCPU = []\n",
    "occK = []\n",
    "cuBlock = (16,16,1) # 512,256,1024\n",
    "threads = cuBlock[0]*cuBlock[1]*cuBlock[2]\n",
    "cuGrid   = None\n",
    "for i in range(8,14):\n",
    "    \n",
    "    \n",
    "    N = 2**i\n",
    "    a = np.ones((N,N),dtype=presCPU)\n",
    "    b = np.ones((N,N),dtype=presCPU)\n",
    "    A_gpu=gpuarray.to_gpu(a)\n",
    "    B_gpu=gpuarray.to_gpu(b)\n",
    "    #cuBlock = (16,16,1) # 512,256,1024\n",
    "    threads = cuBlock[0]*cuBlock[1]*cuBlock[2]\n",
    "    cuGrid   = (N/cuBlock[0],N/cuBlock[1],1)\n",
    "    \n",
    "    myKern  = optKernels(Reg_Glob_RW,pres=presGPU,subBlGr=True,cuB=cuBlock,cuG=cuGrid)\n",
    "    cuCodeRG= SourceModule(myKern)\n",
    "    regRW   = cuCodeRG.get_function(\"rwRegisters\") \n",
    "    gloRW   = cuCodeRG.get_function(\"rwGlobal\")\n",
    "    shaRW   = cuCodeRG.get_function(\"rwShared\")\n",
    "    shaSyRW = cuCodeRG.get_function(\"rwSharedSync\")\n",
    "    conRW   = cuCodeRG.get_function('rwConstant')\n",
    "    #conMemRW = cuCodeRG.get_global('cMem')[0] #Obtenemos un puntero a la direccion de memoria constante declarada en el kernel\n",
    "    regRW.prepare('P')\n",
    "    gloRW.prepare('PP')\n",
    "    shaRW.prepare('P')\n",
    "    shaSyRW.prepare('P')\n",
    "    conRW.prepare('P')\n",
    "    \n",
    "    occK.append([getKernelInfo(regRW,nthreads=threads),getKernelInfo(gloRW,nthreads=threads),\n",
    "                 getKernelInfo(shaRW,nthreads=threads),getKernelInfo(shaSyRW,nthreads=threads),\n",
    "                 getKernelInfo(conRW,nthreads=threads)])\n",
    "    \n",
    "    t1 = 0\n",
    "    t2 = 0\n",
    "    t3 = 0\n",
    "    t4 = 0\n",
    "    t5 = 0\n",
    "    tcpu = 0\n",
    "    for k in range(nLoop):\n",
    "        t_reg = regRW.prepared_timed_call(cuGrid,cuBlock,A_gpu.gpudata)\n",
    "        #ctx.synchronize() \n",
    "        t_glo = gloRW.prepared_timed_call(cuGrid,cuBlock,A_gpu.gpudata,B_gpu.gpudata)\n",
    "        #ctx.synchronize() \n",
    "        t_sha = shaRW.prepared_timed_call(cuGrid,cuBlock,A_gpu.gpudata)\n",
    "        #ctx.synchronize() \n",
    "        t_shaSy = shaSyRW.prepared_timed_call(cuGrid,cuBlock,A_gpu.gpudata)\n",
    "        #ctx.synchronize() \n",
    "        t_con = conRW.prepared_timed_call(cuGrid,cuBlock,A_gpu.gpudata)\n",
    "        #ctx.synchronize()\n",
    "        t = time()\n",
    "        a[:,:] = np.pi\n",
    "        b[:,:] = a\n",
    "        t = time()-t\n",
    "        t1 += t_reg()/nLoop\n",
    "        t2 += t_glo()/nLoop\n",
    "        t3 += t_sha()/nLoop\n",
    "        t4 += t_shaSy()/nLoop\n",
    "        t5 += t_con()/nLoop\n",
    "        tcpu = t / nLoop\n",
    "    timeReg.append(t1)\n",
    "    timeGlo.append(t2)\n",
    "    timeSha.append(t3)\n",
    "    timeShaSy.append(t4)\n",
    "    timeCons.append(t5)\n",
    "    timeCPU.append(tcpu)\n",
    "    A_gpu.gpudata.free()\n",
    "    B_gpu.gpudata.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print myKern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "occK,occK[:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(12,8),dpi=200)\n",
    "plt.semilogx([2**i for i in range(8,14)],np.array(timeReg)*1e-3,'r-.*',label='Register',linewidth=3.0,alpha=0.5)\n",
    "plt.semilogx([2**i for i in range(8,14)],np.array(timeGlo)*1e-3,'b-*',label='Global',linewidth=3.0,alpha=0.9)\n",
    "plt.semilogx([2**i for i in range(8,14)],np.array(timeSha)*1e-3,'g-*',label='Shared',linewidth=3.0,alpha=0.5)\n",
    "plt.semilogx([2**i for i in range(8,14)],np.array(timeShaSy)*1e-3,'m-*',label='Shared Sync',linewidth=3.0,alpha=0.5)\n",
    "plt.semilogx([2**i for i in range(8,14)],np.array(timeCons)*1e-3,'b--o',label='Constant',linewidth=3.0,alpha=0.5)\n",
    "plt.ylabel('Time (sec)')\n",
    "plt.xlabel('Matrix Dimension')\n",
    "plt.xlim(xmin=250,xmax=2**13.1)\n",
    "plt.title('Pure GPU')\n",
    "plt.legend(loc=2,labelspacing=0.5,fancybox=True, handlelength=3.5, borderaxespad=0.25, borderpad=0.25)\n",
    "plt.xticks([2**i for i in range(8,14)], [2**i for i in range(8,14)], rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(12,8),dpi=200)\n",
    "plt.title('{0}, precision {1}, Block:{2}'.format(device.name(),presGPU,cuBlock),size=18)\n",
    "plt.semilogx([2**i for i in range(8,14)],timeCPU,'.-',color=(0,0,0),label='CPU 1 thread',linewidth=2.0,alpha=0.5)\n",
    "plt.semilogx([2**i for i in range(8,14)],1e-3*np.array(timeReg),'r-.*',label='Register',linewidth=3.0,alpha=0.5)\n",
    "plt.semilogx([2**i for i in range(8,14)],1e-3*np.array(timeGlo),'b-*',label='Global',linewidth=3.0,alpha=0.9)\n",
    "plt.semilogx([2**i for i in range(8,14)],1e-3*np.array(timeSha),'g-*',label='Shared',linewidth=3.0,alpha=0.5)\n",
    "plt.semilogx([2**i for i in range(8,14)],1e-3*np.array(timeShaSy),'m-*',label='Shared Sync',linewidth=3.0,alpha=0.5)\n",
    "plt.semilogx([2**i for i in range(8,14)],1e-3*np.array(timeCons),'b--o',label='Constant',linewidth=3.0,alpha=0.5)\n",
    "plt.ylabel('Time (sec)')\n",
    "plt.xlabel('Matrix Dimension')\n",
    "plt.xlim(xmin=250,xmax=2**13.1)\n",
    "plt.legend(loc=2,labelspacing=0.5,fancybox=True, handlelength=3.5, borderaxespad=0.25, borderpad=0.25)\n",
    "plt.xticks([2**i for i in range(8,14)], [2**i for i in range(8,14)], rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(12,8),dpi=200)\n",
    "plt.title('{0}, precision {1}, Block:{2}'.format(device.name(),presGPU,cuBlock),size=18)\n",
    "#plt.semilogx([2**i for i in range(8,14)],timeCPU,'.-',color=(0,0,0),label='CPU 1 thread',linewidth=2.0,alpha=0.5)\n",
    "plt.semilogx([2**i for i in range(8,14)],np.array(timeCPU)/(1e-3*np.array(timeReg)),'r-.*',label='Register',linewidth=3.0,alpha=0.5)\n",
    "plt.semilogx([2**i for i in range(8,14)],np.array(timeCPU)/(1e-3*np.array(timeGlo)),'b-*',label='Global',linewidth=3.0,alpha=0.9)\n",
    "plt.semilogx([2**i for i in range(8,14)],np.array(timeCPU)/(1e-3*np.array(timeSha)),'g-*',label='Shared',linewidth=3.0,alpha=0.5)\n",
    "plt.semilogx([2**i for i in range(8,14)],np.array(timeCPU)/(1e-3*np.array(timeShaSy)),'m-*',label='Shared Sync',linewidth=3.0,alpha=0.5)\n",
    "plt.semilogx([2**i for i in range(8,14)],np.array(timeCPU)/(1e-3*np.array(timeCons)),'b--o',label='Constant',linewidth=3.0,alpha=0.5)\n",
    "plt.ylabel('SpeedUp [times faster than CPU thread]')\n",
    "plt.xlabel('Matrix Dimension')\n",
    "plt.xlim(xmin=250,xmax=2**13.1)\n",
    "plt.legend(loc=2,labelspacing=0.5,fancybox=True, handlelength=3.5, borderaxespad=0.25, borderpad=0.25)\n",
    "plt.xticks([2**i for i in range(8,14)], [2**i for i in range(8,14)], rotation='vertical')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
