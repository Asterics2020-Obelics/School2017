{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# First, let's start with some bash from Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyCUDA basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pycuda\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring your GPU device(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycuda import autoinit\n",
    "from pycuda.tools import DeviceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "specs = DeviceData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Max threads per block = ',specs.max_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Warp size            =', specs.warp_size\n",
    "print 'Warps per MP         =', specs.warps_per_mp\n",
    "print 'Thread Blocks per MP =', specs.thread_blocks_per_mp\n",
    "print 'Registers            =', specs.registers\n",
    "print 'Shared memory        =', specs.shared_memory\n",
    "print 'Granularity ??       =', specs.smem_granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to list devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pycuda.driver as drv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drv.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drv.get_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "devn = drv.Device.count()\n",
    "print 'Localized GPUs =',devn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "devices = []\n",
    "for i in range(devn):\n",
    "    devices.append(drv.Device(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All you want to know about your GPU, but you're afraid to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sp in devices:\n",
    "    print 'Name = ',sp.name()\n",
    "    print 'PCI Bus = ',sp.pci_bus_id()\n",
    "    print 'Compute Capability = ',sp.compute_capability()\n",
    "    print 'Total Memory = ',sp.total_memory()/(2.**20) , 'MBytes'\n",
    "    attr = sp.get_attributes()\n",
    "    for j in range(len(attr.items())):\n",
    "        print attr.items()[j]#,'Bytes (when apply)'\n",
    "    print '------------------'\n",
    "    print '------------------'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAX_THREADS_PER_BLOCK, 1024\n",
    "\n",
    "For example for a 3D mesh (less optimal), we only have available $$8\\times 8\\times 8 = 512 \\,simetric$$ \n",
    " $$8\\times 8\\times 16 = 1024 \\,cilindrical$$\n",
    "block size per dimension = 8 or 16.\n",
    "In 2D case the optimal value is:\n",
    "$$32\\times32 = 1024$$\n",
    "For the 1D case we has $$1024$$\n",
    "\n",
    "\n",
    "MAX_THREADS_PER_MULTIPROCESSOR, $2048 = 4*2^9$\n",
    "\n",
    "If we can take this literally, we can process in one processor about 4 meshes of $8\\times8\\times8$, or four blocks of 3D meshes. With this result, we can evaluate the efficiency comparing cilindrical and symetric performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now your device has .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drv.mem_get_info()[0]/(2.**20),'MB of Free Memory',drv.mem_get_info()[1]/(2.**20),'MB Total Memory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think in array sizes. For example a float of 4 bytes length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Linear max length:', drv.mem_get_info()[0]/(4)\n",
    "print '2D max length    :', np.sqrt(drv.mem_get_info()[0]/(4))\n",
    "print '3D max length    :', np.power(drv.mem_get_info()[0]/(4),1./3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __CUDA__ __C__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://docs.nvidia.com/cuda\"><img src=\"images/CUDA.png\" width=\"30%\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic example. Vector Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To solve the problem from the Figure below, we will present a basic C implementation. Subsequently, you can find a pure CUDA C implementation. As you will see below in order to program the GPU, an interplay between C (that does the bookkeeping) and CUDA (that does the heavylifting) is needed.\n",
    "\n",
    "![Alt text](images/suma.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "int N = 10;\n",
    "float a[N],b[N],c[N];\n",
    "\n",
    "for (int i = 0; i < N; ++i){\n",
    "\ta[i] = i;\n",
    "\tb[i] = 2.0f;\t\n",
    "}\n",
    "\n",
    "for (int i = 0; i < N; ++i){\n",
    "\tc[i]= a[i]+b[i];\t\n",
    "}\n",
    "\n",
    "for (int i = 0; i < N; ++i){\n",
    "\tprintf(\"%f \\n\",c[i]);\t\n",
    "}\n",
    "\n",
    "\n",
    "return 0;\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!g++ cpuAdd.c -o cpua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat cpuAdd.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./cpua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version CUDA C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/cuda3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/CUDAmodelThreads.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "// CUDA Kernel\n",
    "__global__ void vectorAdd(const float *A, const float *B, float *C, int numElements)\n",
    "{\n",
    "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if (i < numElements)\n",
    "    {\n",
    "        C[i] = A[i] + B[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "/**\n",
    " * Host main routine\n",
    " */\n",
    "int main(void)\n",
    "{\n",
    "    int numElements = 15;\n",
    "    size_t size = numElements * sizeof(float);\n",
    "    printf(\"[Vector addition of %d elements]\\n\", numElements);\n",
    "\n",
    "    float a[numElements],b[numElements],c[numElements];\n",
    "    float *a_gpu,*b_gpu,*c_gpu;\n",
    "\n",
    "    cudaMalloc((void **)&a_gpu, size);\n",
    "    cudaMalloc((void **)&b_gpu, size);\n",
    "    cudaMalloc((void **)&c_gpu, size);\n",
    "\n",
    "    for (int i=0;i<numElements;++i ){\n",
    "    \n",
    "    \ta[i] = i*i;\n",
    "    \tb[i] = i;\n",
    "    \n",
    "    }\n",
    "    // Copy the host input vectors A and B in host memory to the device input vectors in\n",
    "    // device memory\n",
    "    printf(\"Copy input data from the host memory to the CUDA device\\n\");\n",
    "    cudaMemcpy(a_gpu, a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(b_gpu, b, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch the Vector Add CUDA Kernel\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid =(numElements + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    printf(\"CUDA kernel launch with %d blocks of %d threads\\n\", blocksPerGrid, threadsPerBlock);\n",
    "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(a_gpu, b_gpu, c_gpu, numElements);\n",
    "\n",
    "    // Copy the device result vector in device memory to the host result vector\n",
    "    // in host memory.\n",
    "    printf(\"Copy output data from the CUDA device to the host memory\\n\");\n",
    "    cudaMemcpy(c, c_gpu, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    for (int i=0;i<numElements;++i ){\n",
    "    \tprintf(\"%f \\n\",c[i]);\n",
    "    }\n",
    "\n",
    "    // Free device global memory\n",
    "    cudaFree(a_gpu);\n",
    "    cudaFree(b_gpu);\n",
    "    cudaFree(c_gpu);\n",
    "    \n",
    "    printf(\"Done\\n\");\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!nvcc gpuAdd.cu -o gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st PyCUDA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycuda import autoinit\n",
    "from pycuda import gpuarray\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aux = range(15)\n",
    "a = np.array(aux).astype(np.float32)\n",
    "b = (a*a).astype(np.float32)\n",
    "c = np.zeros(len(aux)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_gpu = gpuarray.to_gpu(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b_gpu = gpuarray.to_gpu(b)\n",
    "c_gpu = gpuarray.to_gpu(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aux_gpu = a_gpu+b_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aux_gpu.gpudata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(aux_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_gpu,b_gpu,aux_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd PyCUDA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycuda.elementwise import ElementwiseKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_gpu.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myCudaFunc = ElementwiseKernel(arguments = \"float *a, float *b, float *c\",\n",
    "                               operation = \"c[i] = a[i]+b[i]\",\n",
    "                               name = \"mySumK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myCudaFunc(a_gpu,b_gpu,c_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd PyCUDA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cudaCode = open(\"gpuAdd.cu\",\"r\")\n",
    "myCUDACode = cudaCode.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myCode = SourceModule(myCUDACode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importedKernel = myCode.get_function(\"vectorAdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nData = len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nThreadsPerBlock = 256\n",
    "nBlockPerGrid = 1\n",
    "nGridsPerBlock = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_gpu.set(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importedKernel(a_gpu.gpudata,b_gpu.gpudata,c_gpu.gpudata,block=(256,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we can summarize the following:\n",
    "\n",
    "**Kernel**\n",
    ">The CUDA kernel is the elementary function of parallelization. It features an extended C syntax and is the unit of computation that runs in parallel on the thousands of cores that compose a GPU. The kernel can be of one of the following tyes.\n",
    "\n",
    "> __global__ - denotes general CUDA kernel. These functions are called from the host\n",
    "\n",
    "> __device__ - represents a device (GPU) function. - These functions can be called either from __device__ or __global__\n",
    "\n",
    "> __host__ - represents a host (CPU) function.\n",
    "\n",
    "**And how does parallelization work?**\n",
    ">Each time a kernel is called it is necessary to give it a thread distribution (or _threads_) which are organized in blocks (_blocks_) and these in turn in a _grid_ (these can have different dimensions: 1D, 2D, 3D). These threads are copies of the kernel and each is a process to be carried out on the GPU cores, i.e. if we launch a grid with 5 blocks (_gridDim_ = (5,1,1)) with 10 threads per block (_blockDim_ = (10,1,1)), then we will have launched 50 tasks in parallel. Although the kernels to be executed by the threads are copies of the one that we originally wrote, the differentiation is given by the assignment of a counter to each process. The usual way to determine this ** global process index ** is exemplified below:\n",
    "\n",
    "![Alt text](images/CUDAmodelThreads.png)\n",
    "\n",
    ">(**NOTICE** This can change depending on the number of blocks and threads)\n",
    "![Alt text](images/cuda-grid.png)\n",
    "\n",
    ">For our vector sum example we have used the **global process index** so that each thread makes the sum over a different component of the vectors. It is at this point that the parallelization appears, since each thread operates on a different component of the vector.\n",
    "\n",
    "\n",
    "**PyCUDA**\n",
    ">This Python library lets you access Nvidia‘s CUDA parallel computation API from Python. It allows us in principle to do everything we can do with CUDA C, but in a simpler way. One of the virtues of PyCUDA is that is allows us to use the class **GPUArray**, which in turn allows us to easily manage memory, assign of values, perform data transfer between CPU and GPU, etc. This class of pyCUDA maintains the same structure as the **numpy** library, giving developers the same feel as they were using **numpy**.\n",
    "\n",
    ">After initializing the context of pyCUDA we can make use of the class GPUArray. The simplest way to generate an array in the global memory of the GPU is through _gpuarray.to_gpu (), where the value that is passed to the function is a **numpy** array. Although all GPU global memory arrays are linear arrays, the GPUArray class handles the possibility of preserving array dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#<a href=\"http://documen.tician.de/pycuda/\">pyCUDA</a>\n",
    "\n",
    "#<a href=\"http://docs.scipy.org/doc/numpy/reference/\">Numpy</a>\n",
    "\n",
    "#<a href=\"http://docs.nvidia.com/cuda\">CUDA</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/cudaMatrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pycuda import gpuarray, autoinit\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.tools import DeviceData\n",
    "from pycuda.tools import OccupancyRecord as occupancy\n",
    "nDevices = cuda.Device.count()\n",
    "ndev = None\n",
    "for i in range(nDevices):\n",
    "\tdev = cuda.Device( i )\n",
    "\tprint \"  Device {0}: {1}\".format( i, dev.name() )\n",
    "devNumber = 0\n",
    "if nDevices > 1:\n",
    "\tif ndev == None:\n",
    "\t  devNumber = int(raw_input(\"Select device number: \"))\n",
    "\telse:\n",
    "\t  devNumber = ndev\n",
    "dev = cuda.Device( devNumber)\n",
    "cuda.Context.pop()\n",
    "ctxCUDA = dev.make_context()\n",
    "devdata = DeviceData(dev)\n",
    "print \"Using device {0}: {1}\".format( devNumber, dev.name() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the arrays in CPU memory using Numpy, the preffered array handling library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "presCPU, presGPU = np.float32, 'float'\n",
    "#presCPU, presGPU = np.float64, 'double'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_cpu = np.ones((512,512), dtype=presCPU)\n",
    "a_cpu = np.random.random((512,512)).astype(presCPU)\n",
    "b_cpu = np.ones((512,512), dtype=presCPU)\n",
    "b_cpu = np.random.random((512,512)).astype(presCPU)\n",
    "c_cpu = np.zeros((512,512), dtype=presCPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.random?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(a_cpu)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(b_cpu)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now explicitly copy our arrays from the host (CPU) memory space to the device (GPU) memory space. These tasks are achieved using the GPUArray class from PyCUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_gpu = gpuarray.to_gpu(a_cpu)\n",
    "b_gpu = gpuarray.to_gpu(b_cpu)\n",
    "c_gpu = gpuarray.to_gpu(c_cpu)\n",
    "c_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform the matrix summation on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_cpu = %timeit -o c_cpu = a_cpu+b_cpu \n",
    "# notice the very nice %timeit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_cpu = a_cpu+b_cpu\n",
    "c_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the kernel (GPU function) that operates in a parallel manner on the two input matrices and generates the result in the output matrix C. Notice the C-like syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cudaKernel = '''\n",
    "__global__ void matrixAdd(float *A, float *B, float *C)\n",
    "{\n",
    "    int tid_x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    int tid_y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    int tid   = gridDim.x * blockDim.x * tid_y + tid_x;\n",
    "    C[tid] = A[tid] + B[tid];\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print cudaKernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compile and generate a function from the previously written kernel. This is achieved in a simple way using pyCUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myCode = SourceModule(cudaKernel)\n",
    "addMatrix = myCode.get_function(\"matrixAdd\") # The output of get_function is the GPU-compiled function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(addMatrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must decide the distribution of blocks and threads appropriate to:\n",
    "> 1. Get the required number of tasks completed\n",
    "> 2. Choose an organization appropriate for the dimensions of the problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we deal with matrices, ideally is to generate a 2D distribution of threads. This allows us to perform tasks on blocks of the matrices. NOTICE: If we wanted we could only reuse the vector sum code since in fact matrix summation is a special case of the vector summation exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice for this case is more or less simple. Let $N$ be the dimension of the matrix. Since we have 1024 threads available per block, we can divide the array into blocks of $ \\sqrt{1024} = 32 $. We will always try to exploit to the maximum the amount of thread per block when performing simple tasks. The grid will then be ($N/32$,$N/32$), thus (16,16,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cuBlock = (32,32,1) # In PyCUDA it is necessary to type in the 3rd grid dimension as well.\n",
    "cuGrid = (16,16,1)\n",
    "nthreads = cuBlock[0]*cuBlock[1]*cuBlock[2]\n",
    "nthreads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hardware efficiency reasons, we will usually choose dimensions that are multiples of 32 or powers of 2. This is because the blocks are divided into warps (32-thread unit of execution), and these are executed in parallel in the multiprocessors (SM), so if we do not choose these dimensions wisely we might waste precious computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have set the work distribution for our kernel, we have two ways to invoke the GPU kernel. The first is through the direct use of the compiled function::\n",
    "```python\n",
    "kernelFunction(arg1,arg2, ... ,block=(n,m,l),grid=(r,s,t))\n",
    "```\n",
    "The second is through an intermediate step called \"preparation\" :\n",
    "```python\n",
    "kernelFunction.prepare('ABC..') # Each letter corresponds to an input data type of the function, i = int, f = float, P = pointer, ...\n",
    "kernelFunction.prepared_call(grid,block,arg1.gpudata,arg2,...) # When using GPU arrays, they should be passed as pointers with the attribute 'gpudata'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addMatrix(a_gpu,b_gpu,c_gpu,block=cuBlock,grid=cuGrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage with \"preparation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addMatrix.prepare('PPP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addMatrix.prepared_call(cuGrid,cuBlock,a_gpu.gpudata,b_gpu.gpudata,c_gpu.gpudata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"preparation\" way, it is possible to measure the kernel's execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time2 = addMatrix.prepared_timed_call(cuGrid,cuBlock,a_gpu.gpudata,b_gpu.gpudata,c_gpu.gpudata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = c_gpu.get()  #Here we copy the GPU array back to CPU memory\n",
    "c, c_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(c-c_cpu,interpolation='none')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(np.sum(np.abs(c_cpu-c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getKernelInfo(kernel,nthreads, rt=True):\n",
    "    ''' This function returns info about kernels theoretical performance, but warning is not trivial to optimize! '''\n",
    "    shared=kernel.shared_size_bytes\n",
    "    regs=kernel.num_regs\n",
    "    local=kernel.local_size_bytes\n",
    "    const=kernel.const_size_bytes\n",
    "    mbpt=kernel.max_threads_per_block\n",
    "    #threads =  #self.block_size_x* self.block_size_y* self.block_size_z\n",
    "    occupy = occupancy(devdata, nthreads, shared_mem=shared, registers=regs)\n",
    "    print \"==Kernel Memory==\"\n",
    "    print(\"\"\"Local:        {0}\n",
    "Shared:       {1}\n",
    "Registers:    {2}\n",
    "Const:        {3}\n",
    "Max Threads/B:{4}\"\"\".format(local,shared,regs,const,mbpt))\n",
    "    print \"==Occupancy==\"\n",
    "    print(\"\"\"Blocks executed by SM: {0}\n",
    "Limited by:            {1}\n",
    "Warps executed by SM:  {2}\n",
    "Occupancy:             {3}\"\"\".format(occupy.tb_per_mp,occupy.limited_by,occupy.warps_per_mp,occupy.occupancy))\n",
    "    if rt:\n",
    "        return occupy.occupancy\n",
    "    \n",
    "def gpuMesureTime(myKernel, ntimes=1000):\n",
    "    start = cuda.Event()\n",
    "    end = cuda.Event()\n",
    "    start.record()\n",
    "    for i in range(ntimes):\n",
    "      myKernel()\n",
    "    end.record()\n",
    "    end.synchronize()\n",
    "    timeGPU = start.time_till(end)*1e-3\n",
    "    print \"Call the function {0} times takes in GPU {1} seconds.\\n\".format(ntimes,timeGPU)\n",
    "    print \"{0} seconds per call\".format(timeGPU/ntimes)\n",
    "    return timeGPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the performance of our kernel as well as the way in which the calculation is distributed per block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getKernelInfo(addMatrix,nthreads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timeGPU=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time for i in range(1000): timeGPU.append(addMatrix.prepared_timed_call(cuGrid,cuBlock,a_gpu.gpudata,b_gpu.gpudata,c_gpu.gpudata)())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(np.array(timeGPU))*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/matrixMul.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the matrix multiplication case, each thread will calculate an input of the matrix resulting from the multiplication, which implies that each thread will calculate a dot product between a row of matrix A and a column of matrix B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cudaKernel2 = '''\n",
    "__global__ void matrixMul(float *A, float *B, float *C)\n",
    "{\n",
    "    int tid_x = blockDim.x * blockIdx.x + threadIdx.x; // Row\n",
    "    int tid_y = blockDim.y * blockIdx.y + threadIdx.y; // Column\n",
    "    int matrixDim = gridDim.x * blockDim.x;\n",
    "    int tid   = matrixDim * tid_y + tid_x; // element i,j\n",
    "    \n",
    "    float  aux=0.0f;\n",
    "    \n",
    "    for ( int i=0 ; i<matrixDim ; i++ ){\n",
    "        //          \n",
    "        aux += A[matrixDim * tid_y + i]*B[matrixDim * i + tid_x] ;\n",
    "    \n",
    "    }\n",
    "    \n",
    "    C[tid] = aux;\n",
    "             \n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myCode = SourceModule(cudaKernel2)\n",
    "mulMatrix = myCode.get_function(\"matrixMul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mulMatrix(a_gpu,b_gpu,c_gpu,block=cuBlock,grid=cuGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dotAB = np.dot(a_cpu,b_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dotAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff = np.abs(c_gpu.get()-dotAB)\n",
    "np.sum(np.sum(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(diff,interpolation='none')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getKernelInfo(mulMatrix,nthreads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now check the performance of pyCUDA vs. Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, if we use single precision, we use 4 bytes per array element, while in double precision we will have 8 bytes per element. Thus, in double precision the storage requirements for the different configurations are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Points|size 1D (Mb)|size 2D (Mb)|size 3D (Mb)|\n",
    "|:----:|:----------:|:----------:|:----------:|\n",
    "|128|0.001|0.125|16|\n",
    "|256|0.002|0.5|128|\n",
    "|512|0.004|2|1024|\n",
    "|1024|0.008|8|8192|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "def myColorRand():\n",
    "    return (np.random.random(),np.random.random(),np.random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dimension = [2**i for i in range(5,25) ]\n",
    "myPrec = presCPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dimension "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nLoops = 100\n",
    "timeCPU = []\n",
    "for n in dimension:\n",
    "    v1_cpu = np.random.random(n).astype(myPrec)\n",
    "    v2_cpu = np.random.random(n).astype(myPrec)\n",
    "    tMean = 0\n",
    "    for i in range(nLoops):\n",
    "        t = time() \n",
    "        v = v1_cpu+v2_cpu\n",
    "        t = time() - t\n",
    "        tMean += t/nLoops\n",
    "    timeCPU.append(tMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(10,6), dpi=200)\n",
    "plt.semilogx(dimension,timeCPU,'b-*')\n",
    "plt.ylabel('Time (sec)')\n",
    "plt.xlabel('N')\n",
    "plt.xticks(dimension, dimension, rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU version**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the handy GPUArray class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timeGPU1 = []\n",
    "bandWidth1 = []\n",
    "for n in dimension:\n",
    "    v1_cpu = np.random.random(n).astype(myPrec)\n",
    "    v2_cpu = np.random.random(n).astype(myPrec)\n",
    "    t1Mean = 0\n",
    "    t2Mean = 0\n",
    "    for i in range(nLoops):\n",
    "        t = time()\n",
    "        vaux = gpuarray.to_gpu(v1_cpu)\n",
    "        t = time() -t\n",
    "        t1Mean += t/nLoops\n",
    "    bandWidth1.append(t1Mean)\n",
    "    v1_gpu = gpuarray.to_gpu(v1_cpu) \n",
    "    v2_gpu = gpuarray.to_gpu(v2_cpu)\n",
    "    for i in range(nLoops):\n",
    "        t = time()\n",
    "        v = v1_gpu+v2_gpu\n",
    "        t = time() -t\n",
    "        t2Mean += t/nLoops\n",
    "    timeGPU1.append(t2Mean)\n",
    "    v1_gpu.gpudata.free()\n",
    "    v2_gpu.gpudata.free()\n",
    "    v.gpudata.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(10,6))\n",
    "plt.semilogx(dimension,timeGPU1,'r-*',label='GPU Simple')\n",
    "plt.semilogx(dimension,timeCPU,'b-*',label='CPU')\n",
    "plt.ylabel('Time (sec)')\n",
    "plt.xlabel('N')\n",
    "plt.xticks(dimension, dimension, rotation='vertical')\n",
    "plt.legend(loc=1,labelspacing=0.5,fancybox=True, handlelength=1.5, borderaxespad=0.25, borderpad=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(10,6))\n",
    "\n",
    "a = np.array(timeGPU1)\n",
    "b = np.array(timeCPU)\n",
    "plt.semilogx(dimension,b/a,'r-*',label='CPUtime/GPUtime')\n",
    "plt.ylabel('SpeedUp x')\n",
    "plt.xlabel('N')\n",
    "plt.title('SpeedUP')\n",
    "plt.xticks(dimension, dimension, rotation='vertical')\n",
    "plt.legend(loc=1,labelspacing=0.5,fancybox=True, handlelength=1.5, borderaxespad=0.25, borderpad=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory transfer cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(10,6))\n",
    "sizeMB = np.array(dimension)/(2.**20)\n",
    "print sizeMB\n",
    "plt.semilogx(sizeMB,bandWidth1,'m-+',label='GPU copy  HostToDevice')\n",
    "plt.semilogx(sizeMB,timeGPU1,'r-*',label='GPU Simple Sum')\n",
    "plt.ylabel('Time (sec)')\n",
    "plt.xlabel('Memory (MB)')\n",
    "plt.xticks(sizeMB, sizeMB, rotation='vertical')\n",
    "plt.legend(loc=1,labelspacing=0.5,fancybox=True, handlelength=1.5, borderaxespad=0.25, borderpad=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation using the elementwise type functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycuda.elementwise import ElementwiseKernel\n",
    "myCudaFunc = ElementwiseKernel(arguments = \"float *a, float *b, float *c\",\n",
    "                               operation = \"c[i] = a[i]+b[i]\",\n",
    "                               name = \"mySumK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pycuda.driver as drv\n",
    "start = drv.Event()\n",
    "end = drv.Event()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timeGPU2 = []\n",
    "for n in dimension:\n",
    "    v1_cpu = np.random.random(n).astype(myPrec)\n",
    "    v2_cpu = np.random.random(n).astype(myPrec)\n",
    "    v1_gpu = gpuarray.to_gpu(v1_cpu) \n",
    "    v2_gpu = gpuarray.to_gpu(v2_cpu)\n",
    "    vr_gpu  = gpuarray.to_gpu(v2_cpu)\n",
    "    t3Mean=0\n",
    "    for i in range(nLoops):\n",
    "        start.record()\n",
    "        myCudaFunc(v1_gpu,v2_gpu,vr_gpu)\n",
    "        end.record()\n",
    "        end.synchronize()\n",
    "        secs = start.time_till(end)*1e-3\n",
    "        t3Mean+=secs/nLoops\n",
    "    timeGPU2.append(t3Mean)\n",
    "    v1_gpu.gpudata.free()\n",
    "    v2_gpu.gpudata.free()\n",
    "    vr_gpu.gpudata.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(10,6))\n",
    "plt.semilogx(dimension,timeGPU1,'r-*',label='GPU Simple Sum')\n",
    "plt.semilogx(dimension,timeGPU2,'g-*',label='GPU ElementWise Sum')\n",
    "plt.ylabel('Time (sec)')\n",
    "plt.xlabel('N')\n",
    "plt.xticks(dimension, dimension, rotation='vertical')\n",
    "plt.legend(loc=1,labelspacing=0.5,fancybox=True, handlelength=1.5, borderaxespad=0.25, borderpad=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTICE:\n",
    "\n",
    "Both Element-Wise and Simple Sum have no control over the distribution of calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(10,6))\n",
    "plt.title('GPU: {0}, in {1} precision'.format(dev.name(),presGPU),size=22)\n",
    "a=np.array(timeGPU1)\n",
    "b=np.array(timeGPU2)\n",
    "plt.semilogx(dimension,a/b,'r-*',label='GPU SS / GPU EW')\n",
    "plt.ylabel(' Speedup x')\n",
    "plt.xlabel('N')\n",
    "plt.xticks(dimension, dimension, rotation='vertical')\n",
    "plt.legend(loc=1,labelspacing=0.5,fancybox=True, handlelength=1.5, borderaxespad=0.25, borderpad=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(10,6))\n",
    "plt.title('GPU: {0}, in {1} precision'.format(dev.name(),presGPU),size=22)\n",
    "a=np.array(timeCPU)\n",
    "b=np.array(timeGPU2)\n",
    "plt.semilogx(dimension,a/b,'r-*',label='CPU / GPU EW')\n",
    "plt.ylabel(' Speedup x')\n",
    "plt.xlabel('N')\n",
    "plt.xticks(dimension, dimension, rotation='vertical')\n",
    "plt.legend(loc=1,labelspacing=0.5,fancybox=True, handlelength=1.5, borderaxespad=0.25, borderpad=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have PyCUDA kernel-based implementation using SourceModule. In this implementation we can test the variation of the size of the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cudaCode = open(\"gpuAdd.cu\",\"r\")\n",
    "cudaCode = cudaCode.read()\n",
    "cudaCode = cudaCode.replace('float',presGPU )\n",
    "print cudaCode\n",
    "myCode = SourceModule(cudaCode)\n",
    "vectorAddKernel = myCode.get_function(\"vectorAdd\")\n",
    "vectorAddKernel.prepare('PPP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeGPU3 = []\n",
    "occupancyMesure=[]\n",
    "for nt in [32,64,128,256,512,1024]:\n",
    "    aux = []\n",
    "    auxOcc = []\n",
    "    for n in dimension:\n",
    "        v1_cpu = np.random.random(n).astype(myPrec)\n",
    "        v2_cpu = np.random.random(n).astype(myPrec)\n",
    "        v1_gpu = gpuarray.to_gpu(v1_cpu) \n",
    "        v2_gpu = gpuarray.to_gpu(v2_cpu)\n",
    "        vr_gpu  = gpuarray.to_gpu(v2_cpu)\n",
    "        cudaBlock = (nt,1,1) \n",
    "        cudaGrid    = ((n+nt-1)/nt,1,1)\n",
    "        \n",
    "        cudaCode = open(\"gpuAdd.cu\",\"r\")\n",
    "        cudaCode = cudaCode.read()\n",
    "        cudaCode = cudaCode.replace('float',presGPU )\n",
    "        downVar = ['blockDim.x','blockDim.y','blockDim.z','gridDim.x','gridDim.y','gridDim.z']\n",
    "        upVar      = [str(cudaBlock[0]),str(cudaBlock[1]),str(cudaBlock[2]),\n",
    "                     str(cudaGrid[0]),str(cudaGrid[1]),str(cudaGrid[2])]\n",
    "        dicVarOptim = dict(zip(downVar,upVar))\n",
    "        for i in downVar:\n",
    "            cudaCode = cudaCode.replace(i,dicVarOptim[i])\n",
    "        #print cudaCode\n",
    "        myCode = SourceModule(cudaCode)\n",
    "        vectorAddKernel = myCode.get_function(\"vectorAdd\")\n",
    "        vectorAddKernel.prepare('PPP')\n",
    "        \n",
    "        print '\\n Size={0}, threadsPerBlock={1}'.format(n,nt)\n",
    "        print cudaBlock,cudaGrid\n",
    "        t5Mean = 0\n",
    "        for i in range(nLoops):\n",
    "            timeAux = vectorAddKernel.prepared_timed_call(cudaGrid,cudaBlock,v1_gpu.gpudata,v2_gpu.gpudata,vr_gpu.gpudata)\n",
    "            t5Mean += timeAux()/nLoops\n",
    "        auxOcc.append(getKernelInfo(vectorAddKernel,cudaBlock[0]*cudaBlock[1]*cudaBlock[2]))\n",
    "        aux.append(t5Mean)\n",
    "        v1_gpu.gpudata.free()\n",
    "        v2_gpu.gpudata.free()\n",
    "        vr_gpu.gpudata.free()\n",
    "    timeGPU3.append(aux)\n",
    "    occupancyMesure.append(auxOcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timeGPU3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(10,6),dpi=100)\n",
    "plt.semilogx(dimension,timeGPU1,'y-*',label='GPU Simple Sum')\n",
    "plt.semilogx(dimension,timeGPU2,'g-*',label='GPU ElementWise Sum')\n",
    "count = 0\n",
    "for nt in [32,64,128,256,512,1024]:\n",
    "    plt.semilogx(dimension,timeGPU3[count],'-*',label='GPU Kernel, block={0}'.format(nt),color=(0,1./(count+1),1))\n",
    "    count+=1\n",
    "plt.ylabel('Time (sec)')\n",
    "plt.xlabel('N')\n",
    "plt.xticks(dimension, dimension, rotation='vertical')\n",
    "plt.legend(loc=2,labelspacing=0.5,fancybox=True, handlelength=1.5, borderaxespad=0.25, borderpad=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(10,6),dpi=200)\n",
    "count = 0\n",
    "for nt in [32,64,128]:\n",
    "    plt.semilogx(dimension,occupancyMesure[count],'-*',label='GPU Kernel, block={0}'.format(nt),color=(0,1./(2*count+1),0), alpha=0.5)\n",
    "    count+=1\n",
    "plt.ylabel('Occupancy')\n",
    "plt.xlabel('N data')\n",
    "plt.ylim(0,1.2)\n",
    "plt.xticks(dimension, dimension, rotation='vertical')\n",
    "plt.legend(loc=2,labelspacing=0.5,fancybox=True, handlelength=1.5, borderaxespad=0.25, borderpad=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(10,6),dpi=200)\n",
    "count = 3\n",
    "for nt in [256,512,1024]:\n",
    "    plt.semilogx(dimension,occupancyMesure[count],'-*',label='GPU Kernel, block={0}'.format(nt),color=(0.5,1./(2*count+1),1./count), alpha=0.9)\n",
    "    count+=1\n",
    "plt.ylabel('Occupancy')\n",
    "plt.xlabel('N data')\n",
    "plt.ylim(0,1.2)\n",
    "plt.xticks(dimension, dimension, rotation='vertical')\n",
    "plt.legend(loc=2,labelspacing=0.5,fancybox=True, handlelength=1.5, borderaxespad=0.25, borderpad=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(12,8),dpi=300)\n",
    "plt.title('GPU: {0}, in {1} presicion'.format(dev.name(),presGPU),size=22)\n",
    "plt.loglog(dimension,timeGPU1,'y-*',label='GPU Simple Sum')\n",
    "plt.loglog(dimension,timeGPU2,'g-*',label='GPU ElementWise Sum')\n",
    "count = 0\n",
    "for nt in [32,64,128,256,512,1024]:\n",
    "    plt.loglog(dimension,timeGPU3[count],'-*',label='GPU Kernel, block={0}'.format(nt),color=myColorRand())\n",
    "    count+=1\n",
    "plt.ylabel('Time (seg)')\n",
    "plt.xlabel('N')\n",
    "plt.xticks(dimension, dimension, rotation='vertical')\n",
    "plt.legend(loc=2,labelspacing=0.5,fancybox=True, handlelength=1.5, borderaxespad=0.25, borderpad=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTICE**:  It would be better to compute an average over the multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myColorRand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(12,8),dpi=200)\n",
    "plt.loglog(dimension,timeGPU1,'y-*',label='GPU Simple Sum', alpha=0.8,linewidth=3)\n",
    "plt.loglog(dimension,timeGPU2,'g-*',label='GPU ElementWise Sum', alpha=0.8,linewidth=3)\n",
    "count = 0\n",
    "for nt in [32,64,128]:\n",
    "    plt.loglog(dimension,timeGPU3[count],'-*',label='GPU Kernel, block={0}'.format(nt),color=myColorRand(), alpha=0.8,linewidth=3)\n",
    "    count+=1\n",
    "plt.ylabel('Time (seg)')\n",
    "plt.xlabel('N')\n",
    "plt.xticks(dimension, dimension, rotation='vertical')\n",
    "plt.legend(loc=2,labelspacing=0.5,fancybox=True, handlelength=1.5, borderaxespad=0.25, borderpad=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(12,8),dpi=200)\n",
    "plt.title('GPU: {0}, in {1} presicion'.format(dev.name(),presGPU),size=22)\n",
    "#plt.loglog(dimension,timeGPU1,'y-*',label='GPU Simple Sum')\n",
    "plt.loglog(dimension,timeGPU2,'g-*',label='GPU ElementWise Sum',alpha=0.8,linewidth=3)\n",
    "count = 3\n",
    "for nt in [256,512,1024]:\n",
    "    plt.loglog(dimension,timeGPU3[count],'-*',label='GPU Kernel, block={0}'.format(nt),color=myColorRand(), alpha=0.8,linewidth=3)\n",
    "    count+=1\n",
    "plt.ylabel('Time (seg)')\n",
    "plt.xlabel('N')\n",
    "plt.ylim(1e-5,2e-3)\n",
    "plt.xticks(dimension, dimension, rotation='vertical')\n",
    "plt.legend(loc=2,labelspacing=0.5,fancybox=True, handlelength=1.5, borderaxespad=0.25, borderpad=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
